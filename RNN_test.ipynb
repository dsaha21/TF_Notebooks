{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import SimpleRNN\n",
    "# myrnn = SimpleRNN(3)\n",
    "# hidden_state = [0,0,0]\n",
    "\n",
    "# sen = [\"I\", \"love\", \"neural\"]\n",
    "\n",
    "# for w in sen:\n",
    "#     pred, hidden_state = myrnn(w, hidden_state)\n",
    "\n",
    "# next_word = pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/a-practical-guide-to-rnn-and-lstm-in-keras-980f176271bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPoints to note, Keras calls input weight as kernel, the hidden matrix as recurrent_kernel and bias as bias. Now let\\'s go through the parameters exposed by Keras. While the complete list is provided, we will look at some of the relevant ones briefly.\\n\\nThe first and foremost is units which is equal to the size of the output of both kernel and recurrent_kernel. It is also the size of bias term and the size of the hidden term.\\n\\nNext, we have activation which defined the g() function in our formulation. Default is \"tanh\".\\n\\nThen we have {*}_initializer , {*}_regularizer and {*}_constraint parameters each for kernel, recurrent_kernel and bias. These could be ignored if you are not sure about them, as the default values are good enough.\\n\\nuse_bias is a boolean parameter which turns on or off the bias term.\\n\\ndropout and recurrent_dropout is used to apply dropout probability to kernel and recurrent_kernel respectively.\\n\\nreturn_sequence is a boolean parameter. When its \"True\", the output shape of the RNN layer is (timestamp, feature) and when its \"False\" the output is only (features). This means, if its turn on, in output we return the y{t} from all time-steps, and if it\\'s off then we only return 1 y{t} (here from the last time-step). An additional caveat, don\\'t forget to add a TimeDistributed layer or Flatten layer after an RNN with return_sequence turned on before you add a Dense layer.\\n\\ngo_backwards is of boolean type and when its \"True\" the RNN process the data in reverse order. \\nDefault is \"False\"\\n\\nreturn_state is of boolean type and when \"True\" it returns the last state in addition to the output. Default is \"False\".\\nstateful is an important parameter. When turned \"True\", Keras uses the same hidden state across batches for the same sample index. Understand it like this, we train our model for multiple epochs which is like iterations over the complete data. 1 epoch is 1 pass over the complete data. Now each epoch contains multiple batches which in turn contains multiple samples i.e. the individual data. Usually, after running on each sample in a batch, the state of the RNN cell is reset. But if we have prepared the data in a format such that across multiple batches, samples at a particular index are just an extension of the same sentence, we can turn stateful as \"True\" and it will equivalent to training all sentences at once (as one sample). We may do this due to memory constraint and hence if we cannot load complete data at one go. Default is \"False\".\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Points to note, Keras calls input weight as kernel, the hidden matrix as recurrent_kernel and bias as bias. Now let's go through the parameters exposed by Keras. While the complete list is provided, we will look at some of the relevant ones briefly.\n",
    "\n",
    "The first and foremost is units which is equal to the size of the output of both kernel and recurrent_kernel. It is also the size of bias term and the size of the hidden term.\n",
    "\n",
    "Next, we have activation which defined the g() function in our formulation. Default is \"tanh\".\n",
    "\n",
    "Then we have {*}_initializer , {*}_regularizer and {*}_constraint parameters each for kernel, recurrent_kernel and bias. These could be ignored if you are not sure about them, as the default values are good enough.\n",
    "\n",
    "use_bias is a boolean parameter which turns on or off the bias term.\n",
    "\n",
    "dropout and recurrent_dropout is used to apply dropout probability to kernel and recurrent_kernel respectively.\n",
    "\n",
    "return_sequence is a boolean parameter. When its \"True\", the output shape of the RNN layer is (timestamp, feature) and when its \"False\" the output is only (features). This means, if its turn on, in output we return the y{t} from all time-steps, and if it's off then we only return 1 y{t} (here from the last time-step). An additional caveat, don't forget to add a TimeDistributed layer or Flatten layer after an RNN with return_sequence turned on before you add a Dense layer.\n",
    "\n",
    "go_backwards is of boolean type and when its \"True\" the RNN process the data in reverse order. \n",
    "Default is \"False\"\n",
    "\n",
    "return_state is of boolean type and when \"True\" it returns the last state in addition to the output. Default is \"False\".\n",
    "stateful is an important parameter. When turned \"True\", Keras uses the same hidden state across batches for the same sample index. Understand it like this, we train our model for multiple epochs which is like iterations over the complete data. 1 epoch is 1 pass over the complete data. Now each epoch contains multiple batches which in turn contains multiple samples i.e. the individual data. Usually, after running on each sample in a batch, the state of the RNN cell is reset. But if we have prepared the data in a format such that across multiple batches, samples at a particular index are just an extension of the same sentence, we can turn stateful as \"True\" and it will equivalent to training all sentences at once (as one sample). We may do this due to memory constraint and hence if we cannot load complete data at one go. Default is \"False\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 128)               17792     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,921\n",
      "Trainable params: 17,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "#build the model \n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(128, input_shape=(5,10)))\n",
    "model.add(Dense(1))\n",
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Vertical RNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN --> LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "\n",
    "opti = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = w **2 - 10*w*25\n",
    "    tv = [w]\n",
    "    grads = tape.gradient(cost, tv)\n",
    "    opti.apply_gradients(zip(grads, tv))\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.09999933>\n"
     ]
    }
   ],
   "source": [
    "train_step()\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ans ... <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=79.962296>\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    train_step()\n",
    "print(f\"the ans ... {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#weight\n",
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "# x --> array features\n",
    "x = np.array([1.0, -10.0, 25.0], dtype = np.float32)\n",
    "opti = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "def training(X, w, Op):\n",
    "    def cost_f():\n",
    "        return X[0]* w**2 + X[1]*w + X[2]\n",
    "    for i in range(1000):\n",
    "        Op.minimize(cost_f, [w])\n",
    "    return w\n",
    "\n",
    "w = training(x, w, opti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLFiles37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
